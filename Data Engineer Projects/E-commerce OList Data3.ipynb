{"cells": [{"cell_type": "code", "execution_count": 1, "id": "1b01f8dd-cfa5-4488-9f41-d8d19d8e3a28", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/03/27 07:08:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n .appName('Olist Ecommerce Performance Optmization') \\\n .config('spark.executor.memory','6g') \\\n .config('spark.executor.cores','4') \\\n .config('spark.executor.instances','2') \\\n .config('spark.driver.memory','4g') \\\n .config('spark.driver.maxResultSize','2g') \\\n .config('spark.sql.shuffle.partitions','64') \\\n .config('spark.default.parallelism','64') \\\n .config('spark.sql.adaptive.enabled','true') \\\n .config('spark.sql.adaptive.coalescePartition.enabled','true') \\\n .config('spark.sql.autoBroadcastJoinThreshold',20*1024*1024) \\\n .config('spark.sql.files.maxPartitionBytes','64MB') \\\n .config('spark.sql.files.openCostInBytes','2MB') \\\n .config('spark.memory.fraction',0.8) \\\n .config('spark.memory.storageFraction',0.2) \\\n .getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "cd47355e-0a0c-4ffd-809a-def096d42b91", "metadata": {"tags": []}, "outputs": [], "source": "hdfs_path = '/data/olist/'\n "}, {"cell_type": "code", "execution_count": null, "id": "ecb89887-5aa3-4bcb-a35d-bcfc92323778", "metadata": {"tags": []}, "outputs": [], "source": "customers_df = spark.read.csv(hdfs_path + 'olist_customers_dataset.csv',header=True,inferSchema=True)\norders_df = spark.read.csv(hdfs_path + 'olist_orders_dataset.csv',header=True,inferSchema=True)\norder_item_df = spark.read.csv(hdfs_path + 'olist_order_items_dataset.csv',header=True,inferSchema=True)\npayments_df = spark.read.csv(hdfs_path + 'olist_order_payments_dataset.csv',header=True,inferSchema=True)\nreviews_df = spark.read.csv(hdfs_path + 'olist_order_reviews_dataset.csv',header=True,inferSchema=True)\nproducts_df = spark.read.csv(hdfs_path + 'olist_products_dataset.csv',header=True,inferSchema=True)\nsellers_df = spark.read.csv(hdfs_path + 'olist_sellers_dataset.csv',header=True,inferSchema=True)\ngeolocation_df = spark.read.csv(hdfs_path + 'olist_geolocation_dataset.csv',header=True,inferSchema=True)\ncategory_translation_df = spark.read.csv(hdfs_path +'product_category_name_translation.csv',header=True,inferSchema=True)"}, {"cell_type": "code", "execution_count": null, "id": "ea83e2e5-2d7e-4555-9787-babed674e97f", "metadata": {}, "outputs": [], "source": "full_orders_df = spark.read.parquet('/olist/processed/')\nfull_orders_df.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "b97dafdf-8ffe-45be-853d-d6290773525a", "metadata": {}, "outputs": [], "source": "# Broadcast\ncustomers_broadcast_df = broadcast(customers_df)\noptimized_broadcast_join = full_orders_df.join(customers_brodcast_df,'customer_id')"}, {"cell_type": "code", "execution_count": null, "id": "9a38f882-97ae-4e30-91a5-e669034d0dd4", "metadata": {}, "outputs": [], "source": "# Sort and Merge join\nsorted_customers_df = customers_df.sortWithinPartitions('customer_id')\nsorted_orders_df = full_orders_df.sortWithinPartitions('customer_id')\noptimized_merge_full_orders_df = sorted_orders_df.join(sorted_customers_df,'customer_id')"}, {"cell_type": "code", "execution_count": null, "id": "2abd4935-6788-4f4b-97d9-581615497f13", "metadata": {}, "outputs": [], "source": "# Bucket join\n\nbucketed_customers_df = customers_df.repartition(10,'customer_id')\nbucketed_orders_df = full_orders_df.repartition(10,'customer_id')\nbucket_join_df = bucketed_orders_df.join(bucketed_customers_df,'customer_id')"}, {"cell_type": "code", "execution_count": null, "id": "5c541925-40a2-4628-b8d2-3bc9adc772c1", "metadata": {}, "outputs": [], "source": " # Skew Join handling\n    \nskew_handled_join = full_orders_df.join(customers_df.hint('skew'),'customer_id')"}, {"cell_type": "markdown", "id": "2c5648e1-133b-4d96-826a-c5f05f4eabda", "metadata": {}, "source": "**Data Serving**"}, {"cell_type": "code", "execution_count": null, "id": "5c204986-8321-4f7b-924d-2778e06e29fb", "metadata": {}, "outputs": [], "source": "# save as Parquet in hdfs\nfull_orders_df.write.mode('overwrite').parquet('/olist/proc')"}, {"cell_type": "code", "execution_count": null, "id": "862e0a57-312c-4bdf-b998-5cb607f9a232", "metadata": {}, "outputs": [], "source": "# Save is as a parquet in Google cloud storage\nfull_orders_df.write.mode('overwrite').parquet('gs://dataproc-staging-us-central1-543409296464-22tg2lko')"}, {"cell_type": "code", "execution_count": null, "id": "f8cd9f43-fbe0-41a4-abc1-9981a10886a2", "metadata": {}, "outputs": [], "source": "full_orders_df.write.mode('overwrite').saveAsTable('full_order_detail')"}, {"cell_type": "code", "execution_count": null, "id": "b13d04d9-85d2-4c25-882d-24e4ef8a1fa6", "metadata": {}, "outputs": [], "source": "spark.sql('show tables')"}, {"cell_type": "code", "execution_count": null, "id": "c1c07687-30b7-4108-861e-cbd860857d1b", "metadata": {}, "outputs": [], "source": "full_orders_df.write.mode('overwrite').option('header','true').csv('/olist/proc/')"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}